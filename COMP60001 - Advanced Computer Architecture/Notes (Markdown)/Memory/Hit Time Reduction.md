
- - - 

***Hit Time Reduction & Address Translation***

- - - 

***AMAT***

*Average Memory Access Time (AMAT) = Hit Time + Miss Rate x Miss Penalty*

*Ways to Improve AMAT:*
- *Reduce Miss Rate*
- *Reduce Miss Penalty*
- ***Reduce Hit Time***

- - - 

***Pipelining Cache***

- → ***in deeper pipelines -  pipelining the cache improves throughput to the cache
- → ***reduces the average hit time (more accesses per unit time)**

***Example: 8-Stage Pipeline (MIPS R4000)***
- *in 5-stage pipeline → cache access must be completed in a single clock cycle (results are required in the next clock cycle)*
- *8-stage pipeline → deeper pipeline needs a higher clock rate (splitting instruction fetch & data fetch stage into two cycles) → pipelining the D-Cache & I-Cache*
- *first cycle → initiate cache access & second cycle → receive data/instruction from cache*
- → ***reduce cache hit time by pipelining the cache (higher throughput → more cache data accessed per unit time )*

![[Pasted image 20231110164738.png|500]]

![[Pasted image 20231110164846.png|500]]

***load-to-use latency:***
- *→ deeper pipeline = higher clock rate - but potentially **higher load-to-use delay** (resulting in 2 stalls - if dependent instruction immediately after the load)*

***branch latency:***
- → ***higher branch latency** → 3 cycles until branch condition is known → if branch taken - 3 instructions must be flushed from pipeline (even 2 stalls if delayed branch slot is used, as delay slot can only fill a single slot)

***performance:***

- *CPI vs Type of Stalls*

![[Pasted image 20231110165645.png|500]]

- ***load-to-use stalls** → compiler can usually schedule instructions to usefully fill stall slots → not too significant*
- ***branch stalls** → significantly program dependent*
- ***FP result stall**s → significantly program dependent, for floating point workloads, stalls due to waiting for results from FP units account for a significant amount of lost performance*
- ***FP structural stalls** → significantly program dependent, for floating point workloads, stalls due to not enough FP functional units available*

- → ***pipelining cache particularly useful when can fill resulting delay slots with other useful instructions***

- - - 

***Cache Bandwidth***

→ ***cache bandwidth may become a limiting factor with a pipelined cache (particularly if multiple instructions issued per cycle)*** 
→ ***want to increase cache bandwidth & support multiple parallel accesses to cache***

**→ *how can we support multiple parallel accesses to the cache?***
- → *divide the cache into several banks*
- → *map addresses to banks*

- → *duplicate the cache? (store to both duplicates, & load from which cache is idle)*
- → *mutli-ported RAM? (support two reads to different addresses every cycle & therefore two reads to cache)*

- - - 

***Virtual Memory & Address Translation***

- *simple processors access memory directly (e.g. microcontrollers)*
- *→ addresses generated by the processor are used directly to access memory*

*how can we?*
- ***run code in an isolated environment** (so if it fails does not crash the whole system, & does not have total access to the system if malicious)*
- ***run more than one application at a time** (that cannot interfere with each other, or don’t need to know about each other)*
- ***use more memory than DRAM***

- → ***virtualise the addressing of memory*
- → ***address translation***

- - - 

***Removing Address Translation from Critical Path***

**→ *reduce hit time by removing address translation from critical path***

- → *processor operates on virtual addresses for the program operating within its virtual environment*
- → *translated to physical addresses to access memory*

![[Pasted image 20231110171401.png|500]]

- ***PIPT** 
- → *entire memory hierarchy operates on physical addresses after translation

- ***VIVI** 
- → *cache indexed by virtual addresses & rest of memory hierarchy accessed by physical addresses (translation only occurs after a cache miss) 
- *→ **reduced hit time (translation taken out of critical path, can get cache hit without having to perform translation)***
- → ***issues when switching from one process to another** (virtual addresses from one process in cache, while now executing on another process with a different virtual address space)*

- ***VIPT** 
- *→ **hit time of VIVI & correctness of PIPT → translation in parallel with L1 cache access***
- → *L1 access uses low-order bits to generate data + tag, in parallel translation translates page number of address & produces physical translation of page number → page number is to be compared with the tag (in parallel given the data, tag & physical address → can compare tag with physical address)*
- → *if tag + address match → low latency hit time*
- *→ if do not match → go to L2 cache*

- - - 

***Paging***

***Pages:***
- *virtual address space → divided into **pages** of equal size*
- *main memory → divided into **page frames** of the same size*

***Paging Mechanism:***
- → *Address Mapping*
- → *Page Transfer*

![[Pasted image 20240102130733.png|250]]

***Address Mapping:***
- *virtual address space for a process is larger than physical memory available*
- ***translations → map pages in virtual address space to either a physical page in the DRAM or an inactive page on the Hard Drive***
- → *program runs fast most of the time → accessing data present in the real memory*

***Page Transfer:***
- → ***occasionally → accessed a page that is not in main memory → raises an exception to OS → page fault**
- → *exception interrupts processor at precise point & inspects the address tying to access & either decides this is an invalid access or finds this data on the HDD which is then allocated to a vacant page in the real memory, correcting the mapping to the page in the real memory, returning from the exception & continuing execution
- → *another layer of the memory hierarchy*

- - - 

***Address Mapping***

![[Pasted image 20231110173026.png|500]]

***virtual address:***
- *consists of a page number P + offset W (to access a particular word in that page)*

***page table:***
- *in any particular context, processor has pointer to current page table (**mapping from page numbers to physical locations + some metadata about the mapping)***

***virtualisation:***
- *→ **every virtual address P+W mapped through the page table to produce a physical address to access the memory***
- → *when switch to another process → switch pointer to another page table → different mapping*

![[Pasted image 20231110173503.png|500]]

***to speedup:***
- *to speedup → **add some caches & cache address translation***
- ***TLB → cache of the page table** (most of the time, page table translation does not cost too much)*
- → *highly associative & close integration*

- - - 

***Why Address Translation?***

- → ***one computer can run two processes in isolation such that they do not need to know about each other***
- → *two different processor sharing the same physical memory, each with their own virtual address space (starting at 0)*

![[Pasted image 20231110173708.png|300]]

- ***→ virtual address space can be larger than physical address space (may not have enough DRAM or virtual space may be very large)***
- *maps virtual page numbers → physical page frames*

***two processes sharing the same code**: (e.g. to instances of the same processes with different data or processes sharing a shared library)*

![[Pasted image 20231110173953.png|300]]

- *common code for different processes can map to the same physical pages*

***initial page allocation:***
- *when virtual pages are initially allocated → they all share the same physical page, initialised to zero*
- *when a write occurs → a page fault (OS exception) results in a fresh writeable page being allocated (copy on write)*

![[Pasted image 20231110174330.png|300]]

***two different processes sharing the same memory mapped file:***

![[Pasted image 20231110174542.png|300]]

- - - 

***Synonyms & Homonyms in Address Translation***

***homonyms:***
- ***same virtual address points to two different physical addresses in different processes***
- → *if you have a virtually indexed cache, flush it between context switches - or include a process identifier (PID) in the cache tag that switches with the process (so when a tag is checked → the tag & PID must match)*

![[Pasted image 20231110173708.png|300]]

***synonyms:***
- ***different virtual addresses (from the same or different processes) point to the same physical address***
*in a virtually indexed cache:*
- *a physical address could be cached twice under different virtual addresses*
- *updates to one cached copy would not be reflected in the other cached copy (cache consistency problem)*
- *solution: **make sure synonyms cannot co-exist in the cache** (e.g. OS can force synonyms to have the same index bits in a direct mapped cache - page colouring)*

![[Pasted image 20231110174542.png|300]]

- - - 

***Avoiding Translation***

**→ *reduce hit time by removing address translation from critical path***

![[Pasted image 20231110185902.png|300]]

***VIPT → Virtually Indexed, Physically Tagged:***
- ***virtual address → page number + page offset** (commonly 20bits number + 12bit offset)*
- *L1 Cache indexed with the physical (untranslated) portion of address → offset*
- → *can start tag access & translation in parallel*
- → *can compare to physical tag to physical translated page number*

- *if hit → use data from cache (low latency hit time)*
- *if miss → go to L2 cache (next level of memory hierarchy)*

→ ***limits size of the cache to page size (as we can only use the untranslated virtual address bits to index the cache)***

***potential solutions:***

- ***higher associativity***
- → *attractive & common choice
- → *having more ways provides a bigger cache without needing any more bits for indexing*
- → *L1 caches are often highly associative*

![[Pasted image 20231110190713.png|200]]

- ***page colouring***
- → *get the OS to help*
- → *a cache conflict occurs if two cache blocks that have the same tag (physical address) are mapped to two different virtual addresses*
- → *make sure OS never creates a page table mapping with this property*

- - - 

***What if you use some translated bits as index bits?***

![[Pasted image 20231110191147.png|500]]

- - - 

***Associativity Conflicts depend on Address Translation***

*VIPT:*

![[Pasted image 20231110191213.png|300]]

- *L2 Cache is indexed with the translated, physical address*
- → *the **L2 associativity conflicts depend on the virtual to physical mapping** (two virtual addresses map to the physical addresses with the same index bits in the L2 → conflict)*
- → *running the same program on the same data → may have different mapping → may have more/less associativity conflicts*
- → *helpful if the OS could choose non-conflicting pages for frequently accessed data (**page colouring for conflict avoidance**)*
- → *make sure adjacent pages do not map to the same L2 index*

- - - 

***TLBs Example***

![[Pasted image 20231110191642.png|400]]

- - - 

***Working Set:***
→ ***the working set of a program is the set of data + instructions that a program needs to access over a given period of time during its execution***
→ *the working set changes dynamically as the program executes different parts of the code & accesses different data*

***to reduce hit time:***
- ***cache (& then virtual memory) should contain as much of the working set of the program as possible***
- → *minimises accesses to main memory (& then disk)*

- - - 

***Summary***

*to reduce hit time:*
- ***use a really small L1 cache (& larger next level cache)***

- ***use a pipelined cache (improves throughput)***

- ***use a multi-bank cache (improves bandwidth)***

- ***use a direct-mapped cache***

- ***by taking address translation off the critical path (access the TLB in parallel with L1 Cache - VIPT)***
- → *increase associativity to increase cache capacity - as cache size is limited by only using untranslated virtual address bits to index cache)*

- - - 

